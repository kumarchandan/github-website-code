{"componentChunkName":"component---node-modules-gatsby-theme-blog-core-src-templates-post-query-js","path":"/blog/100-days-of-deep-learning/day3-gradient-descent/","result":{"data":{"site":{"siteMetadata":{"title":"Chandan Kumar","social":[{"name":"Twitter","url":"https://twitter.com/enchandan"},{"name":"GitHub","url":"https://github.com/kumarchandan"},{"name":"LinkedIn","url":"https://www.linkedin.com/in/enchandan/"}]}},"blogPost":{"__typename":"MdxBlogPost","id":"a4a6f5b7-8145-5237-853c-0436a1566261","excerpt":"Introduction There are several methods for weight update available. These are called  optimizers . Gradient descent optimization algorithm…","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Gradient Descent\",\n  \"date\": \"2020-10-12T00:00:00.000Z\",\n  \"imageAlt\": \"Day 3\",\n  \"tags\": [\"Back-Propagtion\", \"Gradient Descent\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", {\n    \"id\": \"introduction\"\n  }, \"Introduction\"), mdx(\"p\", null, \"There are several methods for weight update available. These are called \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"optimizers\"), \". Gradient descent optimization algorithm is one of these.\"), mdx(\"h2\", {\n    \"id\": \"definition\"\n  }, \"Definition\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It is an optimization algorithm which finds and updates best values for the parameters of a mathematical function/model which minimizes it\\u2019s loss function (reduces error)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"To find a local minimum of a function, gradient descent takes steps proportional to the negative of the gradient of the function at the current point\\nFrom our previous notes on \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Backpropagation\"), \"\\n$w_x^{new} = w_x - \\\\eta (\\\\frac {\\\\delta Error} {\\\\delta w_x})$\")), mdx(\"p\", null, \"$$\\na^2 + b^2 = c^2\\n$$\"), mdx(\"p\", null, \"To update the $w_x$ , we take the current $w_x$ and subtract the partial derivative of the error function w.r.t. $w_x$\\nWe also multiply the derivative of the error function with a number $\\\\eta$ (usually small like 0.001) called learning rate to control the steps.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Another example:\\nSuppose we have a Linear regression model for a problem and using \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"MSE\"), \" (Mean squared error) as a loss function to calculate the error.\")), mdx(\"p\", null, \"The equation of a linear regression model is:\\n$y = mx + c$\\nwhere x is the input values, m & c are parameters of the equation\"), mdx(\"p\", null, \"After prediction of values on the training dataset, the loss function MSE will calculate the error.\\nNow job of Gradient Descent algorithm comes into action. It reduces the MSE by finding the best values for parameter m & c on the given dataset as values of m & c directly impacts the predictions done by the linear regression equation.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Like linear regression model mentioned above, Gradient descent optimizes other Machine learning algorithms used during training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Always scale the data as Gradient descent converges faster if all the features are at same scale.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Example of Gradient descent optimization algorithms:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Momentum\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Nesterov Accelerated Gradient\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Adagrad\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RMSProp\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Adam & many more\")))), mdx(\"h2\", {\n    \"id\": \"variants-of-gradient-descent\"\n  }, \"Variants of Gradient Descent\"), mdx(\"hr\", null), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Batch Gradient Descent\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Stochastic Gradient Descent\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Mini-batch Gradient Descent\")), mdx(\"h3\", {\n    \"id\": \"1-batch-gradient-descent-full-batch-gradient-descent\"\n  }, \"1. Batch Gradient Descent (Full Batch Gradient Descent)\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It processes all the training data at once.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In real-life ML problems, training data can be very large which will not fit in the memory at once so it is not preferred.\")), mdx(\"h3\", {\n    \"id\": \"2-stochastic-gradient-descent\"\n  }, \"2. Stochastic Gradient Descent\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It picks one instance of input data randomly (hence the name \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"stochastic\"), \") and processes it at each iteration.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is faster as it consumes less memory, but number of iterations required increases which also becomes overhead to performance.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Also, as it picks instances randomly, there is a lot of fluctuation while reaching to global minima.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Always shuffle the training data at the beginning of each epoch. If not shuffled, SGD might pick similar instances on each epoch and will end up not reaching the global minima - It will work on less variations of input data, so obviously right! \\uD83D\\uDE42\")), mdx(\"h3\", {\n    \"id\": \"3-mini-batch-gradient-descent\"\n  }, \"3. Mini-batch Gradient Descent\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It is combination of both the types mentioned above and works best.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It picks input instances by a batch (say 32 at once) and processes them together. It is faster as well as requires less number of iterations.\")));\n}\n;\nMDXContent.isMDXComponent = true;","slug":"/blog/100-days-of-deep-learning/day3-gradient-descent/","title":"Gradient Descent","tags":["Back-Propagtion","Gradient Descent"],"date":"October 12, 2020","image":null,"imageAlt":null,"socialImage":null},"previous":null,"next":{"__typename":"MdxBlogPost","id":"f65ebe09-a65d-5ff4-9156-a8f5867cecc2","excerpt":"LSTMs The idea of LSTMs (Long Short-term Memory Networks) is closely related to the fact that we, humans, understand the context when we…","slug":"/blog/natural-language-processing/LSTMs-GRUs-&-Attention-Blocks/","title":"LSTMs GRUs & Attention Blocks","date":"December 03, 2020"}},"pageContext":{"id":"a4a6f5b7-8145-5237-853c-0436a1566261","nextId":"f65ebe09-a65d-5ff4-9156-a8f5867cecc2","maxWidth":1380}},"staticQueryHashes":["2744905544","3090755652","386998304","764694655"]}