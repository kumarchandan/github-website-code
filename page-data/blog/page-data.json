{"componentChunkName":"component---node-modules-gatsby-theme-blog-core-src-templates-posts-query-js","path":"/blog","result":{"data":{"site":{"siteMetadata":{"title":"Chandan Kumar","social":[{"name":"Twitter","url":"https://twitter.com/enchandan"},{"name":"GitHub","url":"https://github.com/kumarchandan"},{"name":"LinkedIn","url":"https://www.linkedin.com/in/enchandan/"}]}},"allBlogPost":{"nodes":[{"__typename":"MdxBlogPost","id":"f65ebe09-a65d-5ff4-9156-a8f5867cecc2","excerpt":"LSTMs The idea of LSTMs (Long Short-term Memory Networks) is closely related to the fact that we, humans, understand the context when we…","slug":"/blog/natural-language-processing/LSTMs-GRUs-&-Attention-Blocks/","title":"LSTMs GRUs & Attention Blocks","date":"December 03, 2020","tags":[]},{"__typename":"MdxBlogPost","id":"a4a6f5b7-8145-5237-853c-0436a1566261","excerpt":"Introduction There are several methods for weight update available. These are called  optimizers . Gradient descent optimization algorithm…","slug":"/blog/100-days-of-deep-learning/day3-gradient-descent/","title":"Gradient Descent","date":"October 12, 2020","tags":["Back-Propagtion","Gradient Descent"]}]}},"pageContext":{}},"staticQueryHashes":["2744905544","3090755652","386998304","764694655"]}