{"componentChunkName":"component---node-modules-gatsby-theme-blog-core-src-templates-posts-query-js","path":"/blog","result":{"data":{"site":{"siteMetadata":{"title":"Chandan Kumar","social":[{"name":"Twitter","url":"https://twitter.com/enchandan"},{"name":"GitHub","url":"https://github.com/kumarchandan"},{"name":"LinkedIn","url":"https://www.linkedin.com/in/enchandan/"}]}},"allBlogPost":{"nodes":[{"__typename":"MdxBlogPost","id":"bd0cc129-30df-5a1d-8d4e-9fd776fea217","excerpt":"LSTMs The idea of LSTMs (Long Short-term Memory Networks) is closely related to the fact that we, humans, understand the context when we…","slug":"/blog/natural-language-processing/LSTMs-GRUs-&-Attention-Blocks/","title":"LSTMs GRUs & Attention Blocks","date":"December 03, 2020","tags":[]},{"__typename":"MdxBlogPost","id":"aa6e92e8-4c04-54e0-9e1d-bf81745238ab","excerpt":"Introduction There are several methods for weight update available. These are called  optimizers . Gradient descent optimization algorithm…","slug":"/blog/100-days-of-deep-learning/day3-gradient-descent/","title":"Gradient Descent","date":"October 12, 2020","tags":["Back-Propagtion","Gradient Descent"]}]}},"pageContext":{"filter":{},"limit":1000}},"staticQueryHashes":["2744905544","3090755652","386998304","764694655"]}